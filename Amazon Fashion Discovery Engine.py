# -*- coding: utf-8 -*-
"""Amazon Fashion Discovery Engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_a_3RPjLBk-VcVMKThaaD4ERR3kcUT8k

## Importing Packages
"""

from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import warnings
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import math
import time
import re
import os
import itertools
import seaborn as sns
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity  
from sklearn.metrics import pairwise_distances
from matplotlib import gridspec
from scipy.sparse import hstack
import plotly
import plotly.figure_factory as ff
from tqdm import tqdm
from plotly.graph_objs import Scatter, Layout
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import pickle
from IPython.display import display, Image, SVG, Math, YouTubeVideo

plotly.offline.init_notebook_mode(connected=True)
warnings.filterwarnings("ignore")

"""## Analysis of Data  """

data = pd.read_json("/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/Applied_AI_Workshop_Code_Data/tops_fashion.json")

print(f"Number of Data Points: {data.shape[0]}, Number of Features/Variables: {data.shape[1]}")

print(f"Columns of Dataset are: {list(data.columns)}")

"""Columns used in this Case Study are: 

- Amazon Standard Identification Number (asin)
- Brand item belongs to (brand)
- Color (color) 
- Product Type Name (product_type_name)
- Image URL (medium_image_url)
- Title of the Product (title)
- Price of the Product (formatted_price)

"""

data = data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title', 'formatted_price']]

print(f"Number of Data Points: {data.shape[0]}\n Number of Features: {data.shape[1]}")
data.head()

"""### Univariate Analysis

#### Product Type
"""

print(data['product_type_name'].describe())
print(f"\n{((data['product_type_name'].describe()['freq']/data['product_type_name'].describe()['count'])*100):.2f}% of the data belongs to the type {data['product_type_name'].describe()['top']}")

print(f"The Unique types of Products are: {data['product_type_name'].unique()}")

product_type_count = Counter(list(data['product_type_name']))
print(f"The 10 most frequent product types are: \n{product_type_count.most_common(10)}")

"""#### Color"""

print(data['color'].describe())
print(f"\n{((data['color'].describe()['freq']/data['color'].describe()['count'])*100):.2f}% of the data are {data['color'].describe()['top']} in color")

color_count = Counter(list(data['color']))
print(f"The top 10 most frequent colors are: \n{color_count.most_common(10)}")

"""#### Price"""

print(f"{data['formatted_price'].describe()}\nOnly {(data['formatted_price'].describe()['count'])/(data['formatted_price'].shape[0])*100:.2f}% of the data have price information.")

"""#### Title"""

data['title'].head()

"""The total number of datapoints in the dataset is around 180000. Not all of them have price and color information. Omitting these datapoints to reduce the size of the dataset. """

data = data.loc[~data['formatted_price'].isnull()]
print(f"Number of datapoints after removing datapoints without price feature: {data.shape[0]}")

data = data.loc[~data['color'].isnull()]
print(f"Number of datapoints after removing datapoints without color feature: {data.shape[0]}")

"""## Removing Duplicate Elements

### Understanding how duplicates are formed
"""

print(f"Number of products with the same title: {sum(data.duplicated('title'))}")

"""These shirts have the same title, but different sizes.

<table>
<tr> 
<td><img src="https://drive.google.com/file/d/1yimJKabk0EKUcsk7AC2gRMvPm2li78X6/view?usp=sharing",width=100,height=100> :B00AQ4GMCK</td>
<td><img src="/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/dedupe/B00AQ4GMTS.jpeg",width=100,height=100> :B00AQ4GMTS</td>
</tr>
<tr> 
<td><img src="/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/dedupe/B00AQ4GMLQ.jpeg",width=100,height=100> :B00AQ4GMLQ</td>
<td><img src="/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/dedupe/B00AQ4GN3I.jpeg",width=100,height=100> :B00AQ4GN3I</td>
</tr>
</table>

These shirts have the same title, but different colors. 
<table>
<tr> 
<td><img src="dedupe/B00G278GZ6.jpeg",width=100,height=100> :B00G278GZ6</td>
<td><img src="dedupe/B00G278W6O.jpeg",width=100,height=100> :B00G278W6O</td>
</tr>
<tr> 
<td><img src="dedupe/B00G278Z2A.jpeg",width=100,height=100> :B00G278Z2A</td>
<td><img src="dedupe/B00G2786X8.jpeg",width=100,height=100> :B00G2786X8</td>
</tr>
</table>

The dataset contains many such producest which need to be de-duped

### Removing Duplicates
"""

data.head()

data_sorted = data[data['title'].apply(lambda x: len(x.split()) > 4)]
print(f"After removal of products with short description: {data_sorted.shape[0]}")

data_sorted.sort_values('title', inplace = True, ascending = False)
data_sorted.head()

"""Some titles have very similar sentences apart from a few words. We can use a threshold based system to elimnate very similar titles/datapoints. """

data.to_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/28k_apperal_data')

data = pd.read_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/28k_apperal_data')

indices = []
for i, row in data_sorted.iterrows():
  indices.append(i)

"""- Start from the first row
- For each row, compare current row with previous row. 
- If the number of different words is <= 2, store the first index in a list and move to the next row. 
- Repeat until title with > 2 number of words. 
- Update flag values
"""

# non_duplicate_indices = []
# i = 0
# j = 0
# size = data_sorted.shape[0]
# while i < size and j < size:
#   prev_i = i
#   a = data['title'].loc[indices[i]].split()
#   j = i + 1
#   while j < size:
#     b = data['title'].loc[indices[i]].split()
#     length = max(len(a), len(b))
#     count = 0
#     for k in itertools.zip_longest(a, b):
#       if(k[0] == k[1]):
#         count += 1
#     if (length - count) > 2:
#       non_duplicate_indices.append(data_sorted['asin'].loc[indices[i]])
#       i = j
#       break
#     else:
#       j += 1
#   if prev_i == i:
#     break

stage1_dedupe_asins = []
i = 0
j = 0
num_data_points = data_sorted.shape[0]
while i < num_data_points and j < num_data_points:
    previous_i = i
    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']
    a = data['title'].loc[indices[i]].split()
    # search for the similar products sequentially 
    j = i+1
    while j < num_data_points:
        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'Small']
        b = data['title'].loc[indices[j]].split()
        # store the maximum length of two strings
        length = max(len(a), len(b))
        # count is used to store the number of words that are matched in both strings
        count  = 0
        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings
        # example: a =['a', 'b', 'c', 'd']
        # b = ['a', 'b', 'd']
        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]
        for k in itertools.zip_longest(a,b): 
            if (k[0] == k[1]):
                count += 1
        # if the number of words in which both strings differ are > 2 , we are considering it as those two apperals are different
        # if the number of words in which both strings differ are < 2 , we are considering it as those two apperals are same, hence we are ignoring them
        if (length - count) > 2: # number of words in which both sensences differ
            # if both strings are differ by more than 2 words we include the 1st string index
            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])
            # start searching for similar apperals corresponds 2nd string
            i = j
            break
        else:
            j += 1
    if previous_i == i:
        break

data = data.loc[data['asin'].isin(stage1_dedupe_asins)]

print(f"Number of datapoints: {data.shape[0]}")

data.to_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/17k_apparel_data')

"""Some titles have similar sentences but there are some different words as a result of which they are not alphabetically adjacent to each other. To remove such duplicates, we can run a Brute Force algorithm that compares each title to every other title for similarity. An optimal approach could be to use Inverted Indices. """

data = pd.read_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/17k_apparel_data')

def get_similar_title_indices(data):
  indices = []
  for i, row in data.iterrows():
    indices.append(i)

  non_duplicate_indices = []
  while len(indices) != 0:
    i = indices.pop()
    non_duplicate_indices.append(data['asin'].loc[i])
    a = data['title'].loc[i].split()
    for j in indices:
      b = data['title'].loc[j].split()
      length = max(len(a), len(b))
      count = 0
      for k in itertoools.zip_longest(a, b):
        if(k[0] == k[i]):
          count += 1
      if length - count < 3:
        indices.remove(j)
  return non_duplicate_indices

"""non_duplicate_indices = get_similar_title_indices(data)
data = data.loc[data['asin'].isin(non_duplicate_indices)]
print(f"Number of datapoints after removing similar titles: {data.shape[0]}")
data.to_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/16k_apparel_data')

## Text Pre Processing
"""

data = pd.read_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/16k_apperal_data')

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
print(f"List of Stop Words: {stop_words}")

def nlp_preprocessing(total_text, index, column):
  if type(total_text) is not int:
    string = ""
    for words in total_text.split():
      word = ("".join(e for e in words if e.isalnum()))
      word = word.lower()
      if not word in stop_words:
        string += word + " "
    data[column][index] = string

start_time = time.clock()
for index, row in tqdm(data.iterrows()):
  nlp_preprocessing(row['title'], index, 'title')
print(f"\nTime taken to process text: {time.clock() - start_time} seconds")

data.head()

data.to_pickle("/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/preprocessed_16k")

"""Not using Stemming as it showed poor results.

## Text Based Product Similarity
"""

data = pd.read_pickle("/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/preprocessed_16k")
data.head()

def display_img(url, ax, fig):
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  plt.imshow(img)

def plot_heatmap(keys, values, labels, url, text):
  """
  keys   : list of words of recommended product's title
  values : len(values) == len(keys), values(i) represents the occurence of the word keys(i)
  labels : len(labels) == len(keys), the values of labels depend on the model being used
           if model == bag of words, labels(i) = values(i)
           if model == tfidf weighted bag of words, labels(i) = tfidf(keys(i))
           if model == idf weighted bag of words, labels(i) = idf(keys(i))
  url    : product image's url
  """
  gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[4,1]) 
  fig = plt.figure(figsize=(25,3))
  
  # 1st, ploting heat map that represents the count of commonly ocurred words in title2
  ax = plt.subplot(gs[0])
  # it displays a cell in white color if the word is intersection(lis of words of title1 and list of words of title2), in black if not
  ax = sns.heatmap(np.array([values]), annot=np.array([labels]))
  ax.set_xticklabels(keys) # set that axis labels as the words of title
  ax.set_title(text) # apparel title
  
  # 2nd, plotting image of the the apparel
  ax = plt.subplot(gs[1])
  # we don't want any grid lines for image and no labels on x-axis and y-axis
  ax.grid(False)
  ax.set_xticks([])
  ax.set_yticks([])
  
  # we call dispaly_img based with paramete url
  display_img(url, ax, fig)
  
  # displays combine figure ( heat map and image together)
  plt.show()

def plot_heatmap_image(doc_id, vec1, vec2, url, text, model):
  # doc_id : index of the title1
  # vec1 : input apparels's vector, it is of a dict type {word:count}
  # vec2 : recommended apparels's vector, it is of a dict type {word:count}
  # url : apparels image url
  # text: title of recomonded apparel (used to keep title of image)
  # model, it can be any of the models, 
      # 1. bag_of_words
      # 2. tfidf
      # 3. idf

  # we find the common words in both titles, because these only words contribute to the distance between two title vec's
  intersection = set(vec1.keys()) & set(vec2.keys()) 

  # we set the values of non intersecting words to zero, this is just to show the difference in heatmap
  for i in vec2:
      if i not in intersection:
          vec2[i]=0

  # for labeling heatmap, keys contains list of all words in title2
  keys = list(vec2.keys())
  #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 
  values = [vec2[x] for x in vec2.keys()]
  
  # labels: len(labels) == len(keys), the values of labels depends on the model we are using
      # if model == 'bag of words': labels(i) = values(i)
      # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))
      # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))

  if model == 'bag_of_words':
      labels = values
  elif model == 'tfidf':
      labels = []
      for x in vec2.keys():
          # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus
          # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)
          if x in  tfidf_title_vectorizer.vocabulary_:
              labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])
          else:
              labels.append(0)
  elif model == 'idf':
      labels = []
      for x in vec2.keys():
          # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus
          # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)
          if x in  idf_title_vectorizer.vocabulary_:
              labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])
          else:
              labels.append(0)

  plot_heatmap(keys, values, labels, url, text)

def text_to_vector(text):
  """
  This function gets a list of words along with the frequency of each word given some text
  """
  word = re.compile(r'\w+')
  words = word.findall(text)
  # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result
  return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}

def get_result(doc_id, content_a, content_b, url, model):
    text1 = content_a
    text2 = content_b
    
    # vector1 = dict{word11:#count, word12:#count, etc.}
    vector1 = text_to_vector(text1)

    # vector1 = dict{word21:#count, word22:#count, etc.}
    vector2 = text_to_vector(text2)

    plot_heatmap_image(doc_id, vector1, vector2, url, text2, model)

"""### Bag of Words (BoW) on Product Titles"""

title_vectorizer = CountVectorizer()
title_features = title_vectorizer.fit_transform(data['title'])
title_features.get_shape()

def bag_of_words_model(doc_id, num_results):
  pairwise_dist = pairwise_distances(title_features, title_features[doc_id])
  indices = np.argsort(pairwise_dist.flatten())[:num_results]
  pdists = np.sort(pairwise_dist.flatten())[0:num_results]
  df_indices = list(data.index[indices])
  for i in range(0, len(indices)):
    get_result(indices[i],data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'bag_of_words')
    print(f"ASIN {data['asin'].loc[df_indices[i]]}")
    print(f"Brand {data['brand'].loc[df_indices[i]]}")
    print(f"Title {data['title'].loc[df_indices[i]]}")
    print(f"Euclidean similarity with the query image {pdists[i]}")
    print('='*60)

bag_of_words_model(12566, 10)

"""### TFIDF Based Product Similarity"""

tfidf_title_vectorizer = TfidfVectorizer(min_df = 0)
tfidf_title_features = tfidf_title_vectorizer.fit_transform(data['title'])

def tfidf_model(doc_id, num_results):
  pairwise_dist = pairwise_distances(tfidf_title_features, tfidf_title_features[doc_id])
  indices = np.argsort(pairwise_dist.flatten())[0:num_results]
  pdists = np.sort(pairwise_dist.flatten())[0:num_results]
  df_indices = list(data.index[indices])
  for i in range(len(indices)):
    get_result(indices[i], data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'tfidf')
    print(f"ASIN: {data['asin'].loc[df_indices[i]]}")
    print(f"Brand: {data['brand'].loc[df_indices[i]]}")
    print(f"Euclidean Distance from the given image: {pdists[i]}")
    print('='*100)

tfidf_model(12566, 20)

"""### IDF Based Product Similarity"""

idf_title_vectorizer = CountVectorizer()
idf_title_features = idf_title_vectorizer.fit_transform(data['title'])

def n_containing(word):
  """
  This function returns the number of documents(titles) containing the word
  """
  return sum(1 for blob in data['title'] if word in blob.split())

def idf(word):
  return math.log(data.shape[0]/(n_containing(word)))

idf_title_features = idf_title_features.astype(np.float)

for i in tqdm(idf_title_vectorizer.vocabulary_.keys()):
  idf_val = idf(i)
  for j in idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:
    idf_title_features[j, idf_title_vectorizer.vocabulary_[i]] = idf_val

def idf_model(doc_id, num_results):
  pairwise_dist = pairwise_distances(idf_title_features, idf_title_features[doc_id])
  indices = np.argsort(pairwise_dist.flatten())[:num_results]
  pdists = np.sort(pairwise_dist.flatten())[:num_results]
  df_indices = list(data.index[indices])
  for i in range(len(indices)):
    get_result(indices[i], data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'idf')
    print(f"ASIN: {data['asin'].loc[df_indices[i]]}")
    print(f"Brand: {data['brand'].loc[df_indices[i]]}")
    print(f"Euclidean Distance from the given image: {pdists[i]}")
    print('='*100)

idf_model(12566, 20)

"""## Text Semantics Based Product Similarity"""

with open('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/Applied_AI_Workshop_Code_Data/word2vec_model', 'rb') as handle:
  model = pickle.load(handle)

def get_word_vec(sentence, doc_id, m_name):
  vec = []
  for i in sentence.split():
    if i in vocab:
      if m_name == "weighted" and i in idf_title_vectorizer.vocabulary_:
        vec.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[i]] * model[i])
      elif m_name == 'avg':
        vec.append(model[i])
    else:
      vec.append(np.zeros(shape=(300,)))
  return np.array(vec)

def get_distance(vec1, vec2):
  final_dist = []
  for i in vec1:
    dist = []
    for j in vec2:
      dist.append(np.linalg.norm(i-j))
    final_dist.append(np.array(dist))
  return np.array(final_dist)

def heat_map_w2v(sentence1, sentence2, url, doc_id1, doc_id2, model):
  s1_vec = get_word_vec(sentence1, doc_id1, model)
  s2_vec = get_word_vec(sentence2, doc_id2, model)
  s1_s2_dist = get_distance(s1_vec, s2_vec)
  gs = gridspec.GridSpec(2,2, width_ratios = [4,1], height_ratios = [2,1])
  fig = plt.figure(figsize = (15,15))
  ax = plt.subplot(gs[0])
  ax = sns.heatmap(np.round(s1_s2_dist, 4), annot = True)
  ax.set_xticklabels(sentence2.split())
  ax.set_yticklabels(sentence1.split())
  ax.set_title(sentence2)
  ax = plt.subplot(gs[1])
  ax.grid(False)
  ax.set_xticks([])
  ax.set_yticks([])
  display_img(url, ax, fig)
  plt.show()

vocab = model.keys()

def build_avg_vec(sentence, num_features, doc_id, m_name):
  featureVec = np.zeros((num_features,), dtype="float32")
  nwords = 0
  for word in sentence.split():
    nwords += 1
    if word in vocab:
      if m_name == "weighted" and word in idf_title_vectorizer.vocabulary_:
        featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * model[word])
      elif m_name == "avg":
        featureVec = np.add(featureVec, model[word])
  if nwords > 0:
    featureVec = np.divide(featureVec, nwords)
  return featureVec

"""### Average Word2Vec"""

doc_id = 0
w2v_title = []

for i in data['title']:
  w2v_title.append(build_avg_vec(i, 300, doc_id, "avg"))
  doc_id += 1

w2v_title = np.array(w2v_title)

def avg_w2v_model(doc_id, num_results):
  pairwise_dist = pairwise_distances(w2v_title, w2v_title[doc_id].reshape(1, -1))
  indices = np.argsort(pairwise_dist.flatten())[:num_results]
  pdists  = np.sort(pairwise_dist.flatten())[0:num_results]
  df_indices = list(data.index[indices])
  for i in range(len(indices)):
    heat_map_w2v(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i], 'avg')
    print('ASIN :',data['asin'].loc[df_indices[i]])
    print('Brand :',data['brand'].loc[df_indices[i]])
    print ('Euclidean Distance from given input image :', pdists[i])
    print('='*125)

avg_w2v_model(12566, 20)

"""### IDF Weighted Word2Vec"""

doc_id = 0
w2v_title_weight = []
for i in data['title']:
  w2v_title_weight.append(build_avg_vec(i, 300, doc_id, 'weighted'))
  doc_id += 1
w2v_title_weight = np.array(w2v_title_weight)

def weighted_w2v_model(doc_id, num_results):
  pairwise_dist = pairwise_distances(w2v_title_weight, w2v_title_weight[doc_id].reshape(1,-1))
  indices = np.argsort(pairwise_dist.flatten())[:num_results]
  pdists = np.sort(pairwise_dist.flatten())[:num_results]
  df_indices = list(data.index[indices])
  for i in range(len(indices)):
    heat_map_w2v(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i], 'weighted')
    print('ASIN :',data['asin'].loc[df_indices[i]])
    print('Brand :',data['brand'].loc[df_indices[i]])
    print('euclidean distance from input :', pdists[i])
    print('='*125)

weighted_w2v_model(12566, 20)

"""### Weighted Similarity Using Title, Brand and Color"""

data['brand'].fillna(value = "Not Given", inplace =True)

brands = [x.replace(" ", "-") for x in data['brand'].values]
types = [x.replace(" ","-") for x in data['product_type_name'].values]
colors = [x.replace(" ","-") for x in data['color'].values]

brand_vectorizer = CountVectorizer()
brand_features = brand_vectorizer.fit_transform(brands)

type_vectorizer = CountVectorizer()
type_features = type_vectorizer.fit_transform(types)

color_vectorizer = CountVectorizer()
color_features = color_vectorizer.fit_transform(colors)

extra_features = hstack((brand_features, type_features, color_features)).tocsr()

def heatmap_w2v_brand(sentence1, sentence2, url, doc_id1, doc_id2, df_id1, df_id2, model):
  s1_vec = get_word_vec(sentence1, doc_id1, model)
  s2_vec = get_word_vec(sentence2, doc_id2, model)
  s1_s2_dist = get_distance(s1_vec, s2_vec)
  data_matrix = [["ASIN", "Brand", "Color", "Product Type"],
                 [data['asin'].loc[df_id1], brands[doc_id1], colors[doc_id1], types[doc_id1]],
                 [data['asin'].loc[df_id2], brands[doc_id2], colors[doc_id2], types[doc_id2]]]
  colorscale = [[0, "#1D004D"], [.5, "#F2E5FF"], [1, "#F2E5D1"]]
  table = ff.create_table(data_matrix, index=True, colorscale = colorscale)
  plotly.offline.iplot(table, filename = "simple_table")
  gs = gridspec.GridSpec(25,15)
  fig = plt.figure(figsize = (25,5))
  ax1 = plt.subplot(gs[:, :-5])
  ax1 = sns.heatmap(np.round(s1_s2_dist, 6), annot = True)
  ax1.set_xticklabels(sentence2.split())
  ax1.set_yticklabels(sentence1.split())
  ax1.set_title(sentence2)
  ax2 = plt.subplot(gs[:,10:16])
  ax2.grid(False)
  ax2.set_xticks([])
  ax2.set_yticks([])
  display_img(url, ax2, fig)
  plt.show()

def idf_w2v_brand(doc_id, w1, w2, num_results):
  idf_w2v_dist  = pairwise_distances(w2v_title_weight, w2v_title_weight[doc_id].reshape(1,-1))
  ex_feat_dist = pairwise_distances(extra_features, extra_features[doc_id])
  pairwise_dist   = (w1 * idf_w2v_dist +  w2 * ex_feat_dist)/float(w1 + w2)
  indices = np.argsort(pairwise_dist.flatten())[0:num_results]
  pdists  = np.sort(pairwise_dist.flatten())[0:num_results]
  df_indices = list(data.index[indices])
  for i in range(0, len(indices)):
      heatmap_w2v_brand(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i],df_indices[0], df_indices[i], 'weighted')
      print('ASIN :',data['asin'].loc[df_indices[i]])
      print('Brand :',data['brand'].loc[df_indices[i]])
      print('euclidean distance from input :', pdists[i])
      print('='*125)

idf_w2v_brand(12566, 5, 5, 20)

idf_w2v_brand(12566, 5, 50, 20)

"""## Deep Learning Solution

Using vgg-16 to get the dense vectors for each image.
"""

img_width, img_height = 224, 224

top_model_weights_path = 'bottleneck_fc_model.h5'
train_data_dir = 'images2/'
nb_train_samples = 16042
epochs = 50
batch_size = 1


def save_bottlebeck_features():
    
    #Function to compute VGG-16 CNN for image feature extraction.
    
    asins = []
    datagen = ImageDataGenerator(rescale=1. / 255)
    
    # build the VGG16 network
    model = applications.VGG16(include_top=False, weights='imagenet')
    generator = datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode=None,
        shuffle=False)

    for i in generator.filenames:
        asins.append(i[2:-5])

    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size)
    bottleneck_features_train = bottleneck_features_train.reshape((16042,25088))
    
    np.save(open('16k_data_cnn_features.npy', 'wb'), bottleneck_features_train)
    np.save(open('16k_data_cnn_feature_asins.npy', 'wb'), np.array(asins))
    

save_bottlebeck_features()

bottleneck_features_train = np.load('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/Applied_AI_Workshop_Code_Data/16k_data_cnn_features.npy')
asins = np.load('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/Applied_AI_Workshop_Code_Data/16k_data_cnn_feature_asins.npy')
asins = list(asins)

data = pd.read_pickle('/content/drive/MyDrive/AAIC/Case Studies/Amazon Fashion Discovery Engine/pickles/16k_apperal_data_preprocessed')
df_asins = list(data['asin'])

def get_similar_products_cnn(doc_id, num_results):
  doc_id = asins.index(df_asins[doc_id])
  pairwise_dist = pairwise_distances(bottleneck_features_train, bottleneck_features_train[doc_id].reshape(1,-1))
  indices = np.argsort(pairwise_dist.flatten())[:num_results]
  pdists = np.sort(pairwise_dist.flatten())[:num_results]
  for i in range(len(indices)):
    rows = data[['medium_image_url', 'title']].loc[data['asin'] == asins[indices[i]]]
    for indx, row in rows.iterrows():
      display(Image(url=row['medium_image_url'], embed = True))
      print(f"Product Title: {row['title']}")
      print(f"Euclidean Distance from Input Image: {pdists[i]}")
      print(f"Amazon URL: www.amazon.com/dp/{asins[indices[i]]}")

get_similar_products_cnn(12566, 20)

